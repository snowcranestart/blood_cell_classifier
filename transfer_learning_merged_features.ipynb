{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#导入数据包\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import *\n",
    "import os\n",
    "\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设置\n",
    "img_size_1 = (299,299)\n",
    "img_size_2 = (224,224)\n",
    "img_path ='./blood_cell_data/'\n",
    "save_path_inceptionv3 = './merged_features/merged_featurs_inceptionv3.h5'\n",
    "save_path_xception = './merged_features/merged_featurs_xception.h5'\n",
    "save_path_inceptionresnetv2 = './merged_features/merged_featurs_inceptionresnetv2.h5'\n",
    "feature_files = [save_path_inceptionv3,save_path_xception,save_path_inceptionresnetv2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#通过经典模型+GlobalAveragePooling获取特征向量\n",
    "\n",
    "def write_feature_vectors(MODEL,img_size, img_path, save_path,lambda_func):\n",
    "    x = Input((img_size[0],img_size[1], 3))\n",
    "    x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x,weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    print(\"start{}\".format(save_path))\n",
    "    gen = ImageDataGenerator()\n",
    "    features_generator = gen.flow_from_directory(img_path,img_size, shuffle=False,batch_size = 64)\n",
    "    features = model.predict_generator(features_generator,156)\n",
    "    with h5py.File(save_path) as h:\n",
    "        h.create_dataset(\"features\", data = features)\n",
    "        h.create_dataset(\"y\",data = features_generator.classes)\n",
    "    print(\"finished\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选取inceptionv3, xception, inceptionresnetv2获取特征向量并存储在h5文件中\n",
    "#write_feature_vectors(inception_v3.InceptionV3,img_size_1, img_path, save_path_inceptionv3, lambda_func=inception_v3.preprocess_input)        \n",
    "#write_feature_vectors(xception.Xception,img_size_1, img_path, save_path_xception, lambda_func=xception.preprocess_input)\n",
    "#write_feature_vectors(inception_resnet_v2.InceptionResNetV2,img_size_1, img_path, save_path_inceptionresnetv2, lambda_func=inception_resnet_v2.preprocess_input)\n",
    "#生成一次后面注释掉调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#读取三个模型的特征向量，拼接融合在一起   \n",
    "X_features = []    \n",
    "    \n",
    "for filename in feature_files:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_features.append(np.array(h['features']))        \n",
    "        y = np.array(h['y'])\n",
    "                \n",
    "X_features = np.concatenate(X_features, axis=1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "encoded_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "\n",
    "rows = len(y)\n",
    "row_indices = np.random.permutation(rows)\n",
    "\n",
    "split_index = int(rows*0.9)\n",
    "\n",
    "X_train,X_test = X_features[row_indices[0:split_index],:],X_features[row_indices[split_index:],:]\n",
    "\n",
    "encoded_y_train, encoded_y_test = encoded_y[row_indices[0:split_index]],encoded_y[row_indices[split_index:]]\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5632)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5632)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1200)              6759600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1200)              4800      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               307456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 7,090,756\n",
      "Trainable params: 7,087,684\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "#以特征向量为输入，构建神经网络，加BN,Dropout防止过拟合\n",
    "\n",
    "input_tensor = Input(X_train.shape[1:])\n",
    "x = Dropout(0.4)(input_tensor)\n",
    "x = Dense(1200, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(4, activation='softmax')(x)\n",
    "model = Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7168 samples, validate on 1793 samples\n",
      "Epoch 1/700\n",
      "7168/7168 [==============================] - 3s 371us/step - loss: 1.1053 - acc: 0.5306 - val_loss: 0.9758 - val_acc: 0.6308\n",
      "Epoch 2/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.7361 - acc: 0.7083 - val_loss: 0.9278 - val_acc: 0.6648\n",
      "Epoch 3/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.5699 - acc: 0.7803 - val_loss: 0.4736 - val_acc: 0.8109\n",
      "Epoch 4/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.4674 - acc: 0.8209 - val_loss: 0.3632 - val_acc: 0.8555\n",
      "Epoch 5/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.3862 - acc: 0.8623 - val_loss: 0.2915 - val_acc: 0.8912\n",
      "Epoch 6/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.3320 - acc: 0.8764 - val_loss: 0.2706 - val_acc: 0.8924\n",
      "Epoch 7/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.2900 - acc: 0.8979 - val_loss: 0.3074 - val_acc: 0.8840\n",
      "Epoch 8/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.2446 - acc: 0.9118 - val_loss: 0.2626 - val_acc: 0.8974\n",
      "Epoch 9/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.2261 - acc: 0.9187 - val_loss: 0.2218 - val_acc: 0.9180\n",
      "Epoch 10/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.1968 - acc: 0.9301 - val_loss: 0.2127 - val_acc: 0.9258\n",
      "Epoch 11/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.1807 - acc: 0.9346 - val_loss: 0.1805 - val_acc: 0.9320\n",
      "Epoch 12/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.1586 - acc: 0.9438 - val_loss: 0.1672 - val_acc: 0.9392\n",
      "Epoch 13/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.1641 - acc: 0.9414 - val_loss: 0.1451 - val_acc: 0.9487\n",
      "Epoch 14/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.1515 - acc: 0.9445 - val_loss: 0.1754 - val_acc: 0.9314\n",
      "Epoch 15/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.1389 - acc: 0.9489 - val_loss: 0.1250 - val_acc: 0.9515\n",
      "Epoch 16/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.1249 - acc: 0.9565 - val_loss: 0.1539 - val_acc: 0.9437\n",
      "Epoch 17/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.1243 - acc: 0.9562 - val_loss: 0.1680 - val_acc: 0.9403\n",
      "Epoch 18/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.1072 - acc: 0.9622 - val_loss: 0.0929 - val_acc: 0.9637\n",
      "Epoch 19/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.1161 - acc: 0.9586 - val_loss: 0.0996 - val_acc: 0.9654\n",
      "Epoch 20/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.1097 - acc: 0.9636 - val_loss: 0.1500 - val_acc: 0.9515\n",
      "Epoch 21/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.1089 - acc: 0.9623 - val_loss: 0.1044 - val_acc: 0.9615\n",
      "Epoch 22/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.1015 - acc: 0.9634 - val_loss: 0.1085 - val_acc: 0.9632\n",
      "Epoch 23/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.1027 - acc: 0.9633 - val_loss: 0.0981 - val_acc: 0.9626\n",
      "Epoch 24/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.1063 - acc: 0.9629 - val_loss: 0.1248 - val_acc: 0.9543\n",
      "Epoch 25/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0885 - acc: 0.9713 - val_loss: 0.0962 - val_acc: 0.9649\n",
      "Epoch 26/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0852 - acc: 0.9693 - val_loss: 0.1044 - val_acc: 0.9610\n",
      "Epoch 27/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0823 - acc: 0.9713 - val_loss: 0.0865 - val_acc: 0.9671\n",
      "Epoch 28/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0752 - acc: 0.9736 - val_loss: 0.0911 - val_acc: 0.9677\n",
      "Epoch 29/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0810 - acc: 0.9729 - val_loss: 0.0836 - val_acc: 0.9677\n",
      "Epoch 30/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0716 - acc: 0.9750 - val_loss: 0.0917 - val_acc: 0.9626\n",
      "Epoch 31/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0727 - acc: 0.9738 - val_loss: 0.0853 - val_acc: 0.9682\n",
      "Epoch 32/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0621 - acc: 0.9768 - val_loss: 0.1213 - val_acc: 0.9576\n",
      "Epoch 33/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0707 - acc: 0.9761 - val_loss: 0.0760 - val_acc: 0.9727\n",
      "Epoch 34/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0669 - acc: 0.9781 - val_loss: 0.0777 - val_acc: 0.9727\n",
      "Epoch 35/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0690 - acc: 0.9756 - val_loss: 0.0798 - val_acc: 0.9710\n",
      "Epoch 36/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0570 - acc: 0.9805 - val_loss: 0.0800 - val_acc: 0.9721\n",
      "Epoch 37/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0614 - acc: 0.9788 - val_loss: 0.0905 - val_acc: 0.9671\n",
      "Epoch 38/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0704 - acc: 0.9752 - val_loss: 0.0658 - val_acc: 0.9766\n",
      "Epoch 39/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0628 - acc: 0.9787 - val_loss: 0.0817 - val_acc: 0.9682\n",
      "Epoch 40/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0657 - acc: 0.9756 - val_loss: 0.0947 - val_acc: 0.9671\n",
      "Epoch 41/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0591 - acc: 0.9803 - val_loss: 0.0759 - val_acc: 0.9749\n",
      "Epoch 42/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0592 - acc: 0.9782 - val_loss: 0.0749 - val_acc: 0.9721\n",
      "Epoch 43/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0628 - acc: 0.9784 - val_loss: 0.2066 - val_acc: 0.9336\n",
      "Epoch 44/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0514 - acc: 0.9801 - val_loss: 0.0946 - val_acc: 0.9632\n",
      "Epoch 45/700\n",
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0576 - acc: 0.9807 - val_loss: 0.0968 - val_acc: 0.9649\n",
      "Epoch 46/700\n",
      "7168/7168 [==============================] - 1s 90us/step - loss: 0.0562 - acc: 0.9823 - val_loss: 0.0763 - val_acc: 0.9732\n",
      "Epoch 47/700\n",
      "7168/7168 [==============================] - 1s 90us/step - loss: 0.0508 - acc: 0.9835 - val_loss: 0.0906 - val_acc: 0.9660\n",
      "Epoch 48/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0504 - acc: 0.9840 - val_loss: 0.0846 - val_acc: 0.9704\n",
      "Epoch 49/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0498 - acc: 0.9827 - val_loss: 0.0873 - val_acc: 0.9738\n",
      "Epoch 50/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0558 - acc: 0.9807 - val_loss: 0.0927 - val_acc: 0.9710\n",
      "Epoch 51/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0465 - acc: 0.9824 - val_loss: 0.0780 - val_acc: 0.9771\n",
      "Epoch 52/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0469 - acc: 0.9833 - val_loss: 0.0697 - val_acc: 0.9743\n",
      "Epoch 53/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0439 - acc: 0.9831 - val_loss: 0.0822 - val_acc: 0.9727\n",
      "Epoch 54/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0455 - acc: 0.9848 - val_loss: 0.0590 - val_acc: 0.9799\n",
      "Epoch 55/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0503 - acc: 0.9835 - val_loss: 0.0485 - val_acc: 0.9844\n",
      "Epoch 56/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0414 - acc: 0.9842 - val_loss: 0.0464 - val_acc: 0.9844\n",
      "Epoch 57/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0500 - acc: 0.9817 - val_loss: 0.0586 - val_acc: 0.9794\n",
      "Epoch 58/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0426 - acc: 0.9862 - val_loss: 0.0679 - val_acc: 0.9760\n",
      "Epoch 59/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0444 - acc: 0.9852 - val_loss: 0.0593 - val_acc: 0.9771\n",
      "Epoch 60/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0421 - acc: 0.9855 - val_loss: 0.0804 - val_acc: 0.9771\n",
      "Epoch 61/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0434 - acc: 0.9851 - val_loss: 0.0456 - val_acc: 0.9861\n",
      "Epoch 62/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0465 - acc: 0.9849 - val_loss: 0.0902 - val_acc: 0.9710\n",
      "Epoch 63/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0417 - acc: 0.9847 - val_loss: 0.0789 - val_acc: 0.9704\n",
      "Epoch 64/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0418 - acc: 0.9845 - val_loss: 0.0684 - val_acc: 0.9749\n",
      "Epoch 65/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0341 - acc: 0.9872 - val_loss: 0.0847 - val_acc: 0.9743\n",
      "Epoch 66/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0456 - acc: 0.9855 - val_loss: 0.0518 - val_acc: 0.9827\n",
      "Epoch 67/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0419 - acc: 0.9866 - val_loss: 0.0559 - val_acc: 0.9816\n",
      "Epoch 68/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0297 - acc: 0.9893 - val_loss: 0.0601 - val_acc: 0.9788\n",
      "Epoch 69/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0332 - acc: 0.9893 - val_loss: 0.0614 - val_acc: 0.9794\n",
      "Epoch 70/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0387 - acc: 0.9870 - val_loss: 0.0493 - val_acc: 0.9838\n",
      "Epoch 71/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0330 - acc: 0.9887 - val_loss: 0.0454 - val_acc: 0.9827\n",
      "Epoch 72/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0422 - acc: 0.9848 - val_loss: 0.0657 - val_acc: 0.9760\n",
      "Epoch 73/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0377 - acc: 0.9866 - val_loss: 0.0481 - val_acc: 0.9838\n",
      "Epoch 74/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0328 - acc: 0.9881 - val_loss: 0.0543 - val_acc: 0.9794\n",
      "Epoch 75/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0353 - acc: 0.9873 - val_loss: 0.0496 - val_acc: 0.9844\n",
      "Epoch 76/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0382 - acc: 0.9862 - val_loss: 0.0526 - val_acc: 0.9810\n",
      "Epoch 77/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0341 - acc: 0.9888 - val_loss: 0.0641 - val_acc: 0.9794\n",
      "Epoch 78/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0346 - acc: 0.9879 - val_loss: 0.1090 - val_acc: 0.9677\n",
      "Epoch 79/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0318 - acc: 0.9907 - val_loss: 0.0692 - val_acc: 0.9721\n",
      "Epoch 80/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0357 - acc: 0.9877 - val_loss: 0.0707 - val_acc: 0.9782\n",
      "Epoch 81/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0307 - acc: 0.9888 - val_loss: 0.0433 - val_acc: 0.9844\n",
      "Epoch 82/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0326 - acc: 0.9891 - val_loss: 0.0445 - val_acc: 0.9833\n",
      "Epoch 83/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0352 - acc: 0.9862 - val_loss: 0.0521 - val_acc: 0.9799\n",
      "Epoch 84/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0335 - acc: 0.9897 - val_loss: 0.0472 - val_acc: 0.9822\n",
      "Epoch 85/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0296 - acc: 0.9894 - val_loss: 0.0546 - val_acc: 0.9799\n",
      "Epoch 86/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0361 - acc: 0.9907 - val_loss: 0.0817 - val_acc: 0.9677\n",
      "Epoch 87/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0333 - acc: 0.9897 - val_loss: 0.0666 - val_acc: 0.9760\n",
      "Epoch 88/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0307 - acc: 0.9898 - val_loss: 0.0534 - val_acc: 0.9782\n",
      "Epoch 89/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0239 - acc: 0.9916 - val_loss: 0.0587 - val_acc: 0.9810\n",
      "Epoch 90/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.0549 - val_acc: 0.9810\n",
      "Epoch 91/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0309 - acc: 0.9888 - val_loss: 0.0446 - val_acc: 0.9827\n",
      "Epoch 92/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0280 - acc: 0.9908 - val_loss: 0.0417 - val_acc: 0.9866\n",
      "Epoch 93/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0287 - acc: 0.9898 - val_loss: 0.0353 - val_acc: 0.9905\n",
      "Epoch 94/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0302 - acc: 0.9890 - val_loss: 0.0501 - val_acc: 0.9838\n",
      "Epoch 95/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0283 - acc: 0.9909 - val_loss: 0.0521 - val_acc: 0.9810\n",
      "Epoch 96/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0298 - acc: 0.9900 - val_loss: 0.0608 - val_acc: 0.9788\n",
      "Epoch 97/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0241 - acc: 0.9912 - val_loss: 0.0691 - val_acc: 0.9788\n",
      "Epoch 98/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0284 - acc: 0.9905 - val_loss: 0.0753 - val_acc: 0.9716\n",
      "Epoch 99/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0275 - acc: 0.9912 - val_loss: 0.0545 - val_acc: 0.9805\n",
      "Epoch 100/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0236 - acc: 0.9909 - val_loss: 0.0457 - val_acc: 0.9861\n",
      "Epoch 101/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0249 - acc: 0.9907 - val_loss: 0.0487 - val_acc: 0.9833\n",
      "Epoch 102/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0340 - acc: 0.9894 - val_loss: 0.0430 - val_acc: 0.9849\n",
      "Epoch 103/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.0324 - val_acc: 0.9877\n",
      "Epoch 104/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0279 - acc: 0.9911 - val_loss: 0.0381 - val_acc: 0.9888\n",
      "Epoch 105/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0233 - acc: 0.9914 - val_loss: 0.0375 - val_acc: 0.9888\n",
      "Epoch 106/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0253 - acc: 0.9909 - val_loss: 0.0443 - val_acc: 0.9844\n",
      "Epoch 107/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0240 - acc: 0.9925 - val_loss: 0.0470 - val_acc: 0.9844\n",
      "Epoch 108/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0491 - val_acc: 0.9827\n",
      "Epoch 109/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0283 - acc: 0.9898 - val_loss: 0.0477 - val_acc: 0.9838\n",
      "Epoch 110/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0266 - acc: 0.9908 - val_loss: 0.0401 - val_acc: 0.9877\n",
      "Epoch 111/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0295 - acc: 0.9900 - val_loss: 0.0500 - val_acc: 0.9838\n",
      "Epoch 112/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0249 - acc: 0.9907 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "Epoch 113/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0281 - acc: 0.9902 - val_loss: 0.0399 - val_acc: 0.9849\n",
      "Epoch 114/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0293 - acc: 0.9894 - val_loss: 0.0400 - val_acc: 0.9883\n",
      "Epoch 115/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0238 - acc: 0.9914 - val_loss: 0.0500 - val_acc: 0.9866\n",
      "Epoch 116/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0207 - acc: 0.9919 - val_loss: 0.0725 - val_acc: 0.9782\n",
      "Epoch 117/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0280 - acc: 0.9909 - val_loss: 0.0468 - val_acc: 0.9822\n",
      "Epoch 118/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0284 - acc: 0.9902 - val_loss: 0.0480 - val_acc: 0.9822\n",
      "Epoch 119/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0237 - acc: 0.9920 - val_loss: 0.0419 - val_acc: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0204 - acc: 0.9923 - val_loss: 0.0352 - val_acc: 0.9905\n",
      "Epoch 121/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0201 - acc: 0.9939 - val_loss: 0.0501 - val_acc: 0.9849\n",
      "Epoch 122/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0215 - acc: 0.9918 - val_loss: 0.0497 - val_acc: 0.9855\n",
      "Epoch 123/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0494 - val_acc: 0.9827\n",
      "Epoch 124/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0213 - acc: 0.9922 - val_loss: 0.0717 - val_acc: 0.9777\n",
      "Epoch 125/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0205 - acc: 0.9927 - val_loss: 0.0348 - val_acc: 0.9888\n",
      "Epoch 126/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0301 - acc: 0.9902 - val_loss: 0.0597 - val_acc: 0.9788\n",
      "Epoch 127/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0220 - acc: 0.9918 - val_loss: 0.0528 - val_acc: 0.9833\n",
      "Epoch 128/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0261 - acc: 0.9916 - val_loss: 0.0498 - val_acc: 0.9844\n",
      "Epoch 129/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0214 - acc: 0.9919 - val_loss: 0.0395 - val_acc: 0.9861\n",
      "Epoch 130/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0235 - acc: 0.9915 - val_loss: 0.0830 - val_acc: 0.9693\n",
      "Epoch 131/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0283 - acc: 0.9912 - val_loss: 0.0558 - val_acc: 0.9827\n",
      "Epoch 132/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0205 - acc: 0.9926 - val_loss: 0.0545 - val_acc: 0.9833\n",
      "Epoch 133/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0575 - val_acc: 0.9805\n",
      "Epoch 134/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0229 - acc: 0.9922 - val_loss: 0.0466 - val_acc: 0.9833\n",
      "Epoch 135/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0250 - acc: 0.9905 - val_loss: 0.0356 - val_acc: 0.9866\n",
      "Epoch 136/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0256 - acc: 0.9916 - val_loss: 0.0416 - val_acc: 0.9861\n",
      "Epoch 137/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0407 - val_acc: 0.9849\n",
      "Epoch 138/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0165 - acc: 0.9944 - val_loss: 0.0343 - val_acc: 0.9855\n",
      "Epoch 139/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0244 - acc: 0.9933 - val_loss: 0.0476 - val_acc: 0.9822\n",
      "Epoch 140/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0608 - val_acc: 0.9777\n",
      "Epoch 141/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0399 - val_acc: 0.9861\n",
      "Epoch 142/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0155 - acc: 0.9946 - val_loss: 0.0349 - val_acc: 0.9877\n",
      "Epoch 143/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0226 - acc: 0.9923 - val_loss: 0.0432 - val_acc: 0.9855\n",
      "Epoch 144/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0229 - acc: 0.9916 - val_loss: 0.0387 - val_acc: 0.9894\n",
      "Epoch 145/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0188 - acc: 0.9933 - val_loss: 0.0359 - val_acc: 0.9877\n",
      "Epoch 146/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0207 - acc: 0.9923 - val_loss: 0.0347 - val_acc: 0.9838\n",
      "Epoch 147/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0368 - val_acc: 0.9866\n",
      "Epoch 148/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0352 - val_acc: 0.9872\n",
      "Epoch 149/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0199 - acc: 0.9933 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 150/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0172 - acc: 0.9939 - val_loss: 0.0294 - val_acc: 0.9900\n",
      "Epoch 151/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0415 - val_acc: 0.9855\n",
      "Epoch 152/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0188 - acc: 0.9937 - val_loss: 0.0447 - val_acc: 0.9866\n",
      "Epoch 153/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0161 - acc: 0.9933 - val_loss: 0.0445 - val_acc: 0.9861\n",
      "Epoch 154/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0220 - acc: 0.9933 - val_loss: 0.0353 - val_acc: 0.9877\n",
      "Epoch 155/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0263 - acc: 0.9914 - val_loss: 0.0434 - val_acc: 0.9838\n",
      "Epoch 156/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0203 - acc: 0.9934 - val_loss: 0.0405 - val_acc: 0.9855\n",
      "Epoch 157/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0254 - acc: 0.9909 - val_loss: 0.0438 - val_acc: 0.9833\n",
      "Epoch 158/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0182 - acc: 0.9934 - val_loss: 0.0362 - val_acc: 0.9855\n",
      "Epoch 159/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0249 - acc: 0.9907 - val_loss: 0.0397 - val_acc: 0.9833\n",
      "Epoch 160/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0208 - acc: 0.9932 - val_loss: 0.0474 - val_acc: 0.9805\n",
      "Epoch 161/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0613 - val_acc: 0.9766\n",
      "Epoch 162/700\n",
      "7168/7168 [==============================] - 1s 87us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0417 - val_acc: 0.9855\n",
      "Epoch 163/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0200 - acc: 0.9934 - val_loss: 0.0407 - val_acc: 0.9872\n",
      "Epoch 164/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0177 - acc: 0.9940 - val_loss: 0.0324 - val_acc: 0.9905\n",
      "Epoch 165/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0224 - acc: 0.9932 - val_loss: 0.0436 - val_acc: 0.9844\n",
      "Epoch 166/700\n",
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0178 - acc: 0.9943 - val_loss: 0.0346 - val_acc: 0.9844\n",
      "Epoch 167/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0312 - val_acc: 0.9855\n",
      "Epoch 168/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0168 - acc: 0.9940 - val_loss: 0.0326 - val_acc: 0.9888\n",
      "Epoch 169/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0187 - acc: 0.9937 - val_loss: 0.0363 - val_acc: 0.9855\n",
      "Epoch 170/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0166 - acc: 0.9940 - val_loss: 0.0340 - val_acc: 0.9894\n",
      "Epoch 171/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0169 - acc: 0.9936 - val_loss: 0.0285 - val_acc: 0.9905\n",
      "Epoch 172/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0174 - acc: 0.9946 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "Epoch 173/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0166 - acc: 0.9946 - val_loss: 0.0310 - val_acc: 0.9877\n",
      "Epoch 174/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0150 - acc: 0.9950 - val_loss: 0.0495 - val_acc: 0.9849\n",
      "Epoch 175/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0215 - acc: 0.9925 - val_loss: 0.0309 - val_acc: 0.9877\n",
      "Epoch 176/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0196 - acc: 0.9944 - val_loss: 0.0285 - val_acc: 0.9888\n",
      "Epoch 177/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0191 - acc: 0.9932 - val_loss: 0.0364 - val_acc: 0.9900\n",
      "Epoch 178/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0145 - acc: 0.9946 - val_loss: 0.0351 - val_acc: 0.9888\n",
      "Epoch 179/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0156 - acc: 0.9936 - val_loss: 0.0390 - val_acc: 0.9888\n",
      "Epoch 180/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0132 - acc: 0.9953 - val_loss: 0.0373 - val_acc: 0.9866\n",
      "Epoch 181/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0172 - acc: 0.9934 - val_loss: 0.0304 - val_acc: 0.9894\n",
      "Epoch 182/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0166 - acc: 0.9932 - val_loss: 0.0310 - val_acc: 0.9877\n",
      "Epoch 183/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0273 - val_acc: 0.9883\n",
      "Epoch 184/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0225 - acc: 0.9920 - val_loss: 0.0229 - val_acc: 0.9888\n",
      "Epoch 185/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0178 - acc: 0.9927 - val_loss: 0.0354 - val_acc: 0.9877\n",
      "Epoch 186/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0170 - acc: 0.9941 - val_loss: 0.0476 - val_acc: 0.9805\n",
      "Epoch 187/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0196 - acc: 0.9933 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 188/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0148 - acc: 0.9946 - val_loss: 0.0492 - val_acc: 0.9810\n",
      "Epoch 189/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0157 - acc: 0.9947 - val_loss: 0.0359 - val_acc: 0.9866\n",
      "Epoch 190/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0180 - acc: 0.9943 - val_loss: 0.0311 - val_acc: 0.9872\n",
      "Epoch 191/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0168 - acc: 0.9943 - val_loss: 0.0251 - val_acc: 0.9894\n",
      "Epoch 192/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0174 - acc: 0.9934 - val_loss: 0.0455 - val_acc: 0.9810\n",
      "Epoch 193/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0279 - val_acc: 0.9900\n",
      "Epoch 194/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0163 - acc: 0.9941 - val_loss: 0.0356 - val_acc: 0.9883\n",
      "Epoch 195/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0205 - acc: 0.9933 - val_loss: 0.0359 - val_acc: 0.9872\n",
      "Epoch 196/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0151 - acc: 0.9946 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 197/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0194 - acc: 0.9926 - val_loss: 0.0231 - val_acc: 0.9900\n",
      "Epoch 198/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0144 - acc: 0.9948 - val_loss: 0.0455 - val_acc: 0.9855\n",
      "Epoch 199/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0127 - acc: 0.9950 - val_loss: 0.0320 - val_acc: 0.9849\n",
      "Epoch 200/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0188 - acc: 0.9934 - val_loss: 0.0290 - val_acc: 0.9861\n",
      "Epoch 201/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0165 - acc: 0.9943 - val_loss: 0.0494 - val_acc: 0.9810\n",
      "Epoch 202/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.0441 - val_acc: 0.9855\n",
      "Epoch 203/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0150 - acc: 0.9947 - val_loss: 0.0457 - val_acc: 0.9872\n",
      "Epoch 204/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0309 - val_acc: 0.9900\n",
      "Epoch 205/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0138 - acc: 0.9950 - val_loss: 0.0303 - val_acc: 0.9877\n",
      "Epoch 206/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0165 - acc: 0.9941 - val_loss: 0.0337 - val_acc: 0.9883\n",
      "Epoch 207/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0135 - acc: 0.9948 - val_loss: 0.0315 - val_acc: 0.9900\n",
      "Epoch 208/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0140 - acc: 0.9941 - val_loss: 0.0405 - val_acc: 0.9838\n",
      "Epoch 209/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.0406 - val_acc: 0.9877\n",
      "Epoch 210/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0290 - val_acc: 0.9905\n",
      "Epoch 211/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0152 - acc: 0.9947 - val_loss: 0.0222 - val_acc: 0.9911\n",
      "Epoch 212/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0150 - acc: 0.9939 - val_loss: 0.0359 - val_acc: 0.9883\n",
      "Epoch 213/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.0294 - val_acc: 0.9916\n",
      "Epoch 214/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 0.0309 - val_acc: 0.9883\n",
      "Epoch 215/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0154 - acc: 0.9944 - val_loss: 0.0380 - val_acc: 0.9877\n",
      "Epoch 216/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0347 - val_acc: 0.9894\n",
      "Epoch 217/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0156 - acc: 0.9950 - val_loss: 0.0367 - val_acc: 0.9888\n",
      "Epoch 218/700\n",
      "7168/7168 [==============================] - 1s 87us/step - loss: 0.0103 - acc: 0.9962 - val_loss: 0.0426 - val_acc: 0.9888\n",
      "Epoch 219/700\n",
      "7168/7168 [==============================] - 1s 90us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0298 - val_acc: 0.9888\n",
      "Epoch 220/700\n",
      "7168/7168 [==============================] - 1s 87us/step - loss: 0.0138 - acc: 0.9953 - val_loss: 0.0368 - val_acc: 0.9872\n",
      "Epoch 221/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0165 - acc: 0.9944 - val_loss: 0.0482 - val_acc: 0.9838\n",
      "Epoch 222/700\n",
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0396 - val_acc: 0.9855\n",
      "Epoch 223/700\n",
      "7168/7168 [==============================] - 1s 87us/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0415 - val_acc: 0.9838\n",
      "Epoch 224/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0130 - acc: 0.9947 - val_loss: 0.0404 - val_acc: 0.9855\n",
      "Epoch 225/700\n",
      "7168/7168 [==============================] - 1s 87us/step - loss: 0.0181 - acc: 0.9954 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 226/700\n",
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0104 - acc: 0.9961 - val_loss: 0.0432 - val_acc: 0.9872\n",
      "Epoch 227/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0304 - val_acc: 0.9877\n",
      "Epoch 228/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0174 - acc: 0.9937 - val_loss: 0.0492 - val_acc: 0.9822\n",
      "Epoch 229/700\n",
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0147 - acc: 0.9951 - val_loss: 0.0337 - val_acc: 0.9883\n",
      "Epoch 230/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0124 - acc: 0.9946 - val_loss: 0.0382 - val_acc: 0.9883\n",
      "Epoch 231/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0115 - acc: 0.9957 - val_loss: 0.0337 - val_acc: 0.9877\n",
      "Epoch 232/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0147 - acc: 0.9948 - val_loss: 0.0385 - val_acc: 0.9855\n",
      "Epoch 233/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0135 - acc: 0.9951 - val_loss: 0.0361 - val_acc: 0.9861\n",
      "Epoch 234/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0145 - acc: 0.9950 - val_loss: 0.0394 - val_acc: 0.9861\n",
      "Epoch 235/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0149 - acc: 0.9951 - val_loss: 0.0345 - val_acc: 0.9894\n",
      "Epoch 236/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0373 - val_acc: 0.9888\n",
      "Epoch 237/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0454 - val_acc: 0.9888\n",
      "Epoch 238/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0149 - acc: 0.9943 - val_loss: 0.0320 - val_acc: 0.9905\n",
      "Epoch 239/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0126 - acc: 0.9954 - val_loss: 0.0335 - val_acc: 0.9916\n",
      "Epoch 240/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.0356 - val_acc: 0.9883\n",
      "Epoch 241/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0213 - val_acc: 0.9933\n",
      "Epoch 242/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0308 - val_acc: 0.9872\n",
      "Epoch 243/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0141 - acc: 0.9948 - val_loss: 0.0366 - val_acc: 0.9855\n",
      "Epoch 244/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0155 - acc: 0.9943 - val_loss: 0.0222 - val_acc: 0.9911\n",
      "Epoch 245/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0114 - acc: 0.9961 - val_loss: 0.0233 - val_acc: 0.9922\n",
      "Epoch 246/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.0244 - val_acc: 0.9894\n",
      "Epoch 247/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0128 - acc: 0.9958 - val_loss: 0.0202 - val_acc: 0.9916\n",
      "Epoch 248/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0267 - val_acc: 0.9905\n",
      "Epoch 249/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 0.0307 - val_acc: 0.9900\n",
      "Epoch 250/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0241 - val_acc: 0.9911\n",
      "Epoch 251/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0120 - acc: 0.9953 - val_loss: 0.0310 - val_acc: 0.9894\n",
      "Epoch 252/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0461 - val_acc: 0.9822\n",
      "Epoch 253/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0136 - acc: 0.9962 - val_loss: 0.0484 - val_acc: 0.9844\n",
      "Epoch 254/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0285 - val_acc: 0.9911\n",
      "Epoch 255/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0105 - acc: 0.9957 - val_loss: 0.0472 - val_acc: 0.9861\n",
      "Epoch 256/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0138 - acc: 0.9951 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 257/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0145 - acc: 0.9958 - val_loss: 0.0236 - val_acc: 0.9922\n",
      "Epoch 258/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0151 - acc: 0.9950 - val_loss: 0.0328 - val_acc: 0.9888\n",
      "Epoch 259/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0131 - acc: 0.9955 - val_loss: 0.0300 - val_acc: 0.9894\n",
      "Epoch 260/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0136 - acc: 0.9951 - val_loss: 0.0327 - val_acc: 0.9894\n",
      "Epoch 261/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0147 - acc: 0.9951 - val_loss: 0.0323 - val_acc: 0.9855\n",
      "Epoch 262/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0156 - acc: 0.9947 - val_loss: 0.0339 - val_acc: 0.9883\n",
      "Epoch 263/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0152 - acc: 0.9941 - val_loss: 0.0242 - val_acc: 0.9927\n",
      "Epoch 264/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0152 - acc: 0.9950 - val_loss: 0.0293 - val_acc: 0.9900\n",
      "Epoch 265/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0144 - acc: 0.9958 - val_loss: 0.0330 - val_acc: 0.9900\n",
      "Epoch 266/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0140 - acc: 0.9951 - val_loss: 0.0401 - val_acc: 0.9888\n",
      "Epoch 267/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0141 - acc: 0.9955 - val_loss: 0.0290 - val_acc: 0.9916\n",
      "Epoch 268/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0116 - acc: 0.9958 - val_loss: 0.0338 - val_acc: 0.9927\n",
      "Epoch 269/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0146 - acc: 0.9948 - val_loss: 0.0228 - val_acc: 0.9916\n",
      "Epoch 270/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0224 - val_acc: 0.9927\n",
      "Epoch 271/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0283 - val_acc: 0.9900\n",
      "Epoch 272/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0124 - acc: 0.9951 - val_loss: 0.0440 - val_acc: 0.9849\n",
      "Epoch 273/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0352 - val_acc: 0.9866\n",
      "Epoch 274/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0164 - acc: 0.9944 - val_loss: 0.0256 - val_acc: 0.9900\n",
      "Epoch 275/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0201 - val_acc: 0.9927\n",
      "Epoch 276/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0122 - acc: 0.9955 - val_loss: 0.0318 - val_acc: 0.9872\n",
      "Epoch 277/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0100 - acc: 0.9958 - val_loss: 0.0214 - val_acc: 0.9927\n",
      "Epoch 278/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0261 - val_acc: 0.9922\n",
      "Epoch 279/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0134 - acc: 0.9955 - val_loss: 0.0323 - val_acc: 0.9911\n",
      "Epoch 280/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0319 - val_acc: 0.9916\n",
      "Epoch 281/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0138 - acc: 0.9961 - val_loss: 0.0204 - val_acc: 0.9933\n",
      "Epoch 282/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0245 - val_acc: 0.9888\n",
      "Epoch 283/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0119 - acc: 0.9958 - val_loss: 0.0340 - val_acc: 0.9888\n",
      "Epoch 284/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0116 - acc: 0.9964 - val_loss: 0.0338 - val_acc: 0.9861\n",
      "Epoch 285/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0433 - val_acc: 0.9849\n",
      "Epoch 286/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0157 - acc: 0.9950 - val_loss: 0.0360 - val_acc: 0.9872\n",
      "Epoch 287/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0396 - val_acc: 0.9877\n",
      "Epoch 288/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0137 - acc: 0.9953 - val_loss: 0.0308 - val_acc: 0.9905\n",
      "Epoch 289/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0114 - acc: 0.9969 - val_loss: 0.0223 - val_acc: 0.9911\n",
      "Epoch 290/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0228 - val_acc: 0.9922\n",
      "Epoch 291/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0117 - acc: 0.9958 - val_loss: 0.0220 - val_acc: 0.9911\n",
      "Epoch 292/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0312 - val_acc: 0.9877\n",
      "Epoch 293/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0328 - val_acc: 0.9888\n",
      "Epoch 294/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0112 - acc: 0.9968 - val_loss: 0.0221 - val_acc: 0.9927\n",
      "Epoch 295/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0161 - val_acc: 0.9922\n",
      "Epoch 296/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0244 - val_acc: 0.9905\n",
      "Epoch 297/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0312 - val_acc: 0.9877\n",
      "Epoch 298/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0297 - val_acc: 0.9894\n",
      "Epoch 299/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0395 - val_acc: 0.9883\n",
      "Epoch 300/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0242 - val_acc: 0.9922\n",
      "Epoch 301/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0303 - val_acc: 0.9888\n",
      "Epoch 302/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0136 - acc: 0.9960 - val_loss: 0.0442 - val_acc: 0.9872\n",
      "Epoch 303/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0178 - acc: 0.9954 - val_loss: 0.0379 - val_acc: 0.9883\n",
      "Epoch 304/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.0286 - val_acc: 0.9888\n",
      "Epoch 305/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0101 - acc: 0.9962 - val_loss: 0.0326 - val_acc: 0.9900\n",
      "Epoch 306/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.0406 - val_acc: 0.9861\n",
      "Epoch 307/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0130 - acc: 0.9950 - val_loss: 0.0332 - val_acc: 0.9911\n",
      "Epoch 308/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0133 - acc: 0.9954 - val_loss: 0.0419 - val_acc: 0.9883\n",
      "Epoch 309/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0090 - acc: 0.9967 - val_loss: 0.0205 - val_acc: 0.9939\n",
      "Epoch 310/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0126 - acc: 0.9962 - val_loss: 0.0198 - val_acc: 0.9944\n",
      "Epoch 311/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0149 - acc: 0.9948 - val_loss: 0.0290 - val_acc: 0.9905\n",
      "Epoch 312/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0122 - acc: 0.9961 - val_loss: 0.0360 - val_acc: 0.9900\n",
      "Epoch 313/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0461 - val_acc: 0.9833\n",
      "Epoch 314/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0113 - acc: 0.9967 - val_loss: 0.0240 - val_acc: 0.9933\n",
      "Epoch 315/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0084 - acc: 0.9967 - val_loss: 0.0226 - val_acc: 0.9916\n",
      "Epoch 316/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0200 - val_acc: 0.9939\n",
      "Epoch 317/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0128 - acc: 0.9954 - val_loss: 0.0380 - val_acc: 0.9883\n",
      "Epoch 318/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0104 - acc: 0.9967 - val_loss: 0.0204 - val_acc: 0.9916\n",
      "Epoch 319/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0447 - val_acc: 0.9844\n",
      "Epoch 320/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0088 - acc: 0.9967 - val_loss: 0.0241 - val_acc: 0.9933\n",
      "Epoch 321/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0149 - acc: 0.9960 - val_loss: 0.0239 - val_acc: 0.9888\n",
      "Epoch 322/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.0365 - val_acc: 0.9888\n",
      "Epoch 323/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0090 - acc: 0.9967 - val_loss: 0.0357 - val_acc: 0.9883\n",
      "Epoch 324/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0399 - val_acc: 0.9872\n",
      "Epoch 325/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0111 - acc: 0.9961 - val_loss: 0.0316 - val_acc: 0.9888\n",
      "Epoch 326/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0336 - val_acc: 0.9888\n",
      "Epoch 327/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.0261 - val_acc: 0.9888\n",
      "Epoch 328/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0111 - acc: 0.9958 - val_loss: 0.0364 - val_acc: 0.9872\n",
      "Epoch 329/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.0366 - val_acc: 0.9877\n",
      "Epoch 330/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 331/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0150 - acc: 0.9954 - val_loss: 0.0301 - val_acc: 0.9911\n",
      "Epoch 332/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0139 - acc: 0.9953 - val_loss: 0.0439 - val_acc: 0.9855\n",
      "Epoch 333/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0098 - acc: 0.9973 - val_loss: 0.0298 - val_acc: 0.9911\n",
      "Epoch 334/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0112 - acc: 0.9962 - val_loss: 0.0234 - val_acc: 0.9900\n",
      "Epoch 335/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0130 - acc: 0.9953 - val_loss: 0.0282 - val_acc: 0.9888\n",
      "Epoch 336/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.0322 - val_acc: 0.9877\n",
      "Epoch 337/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0100 - acc: 0.9962 - val_loss: 0.0275 - val_acc: 0.9894\n",
      "Epoch 338/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0113 - acc: 0.9964 - val_loss: 0.0283 - val_acc: 0.9911\n",
      "Epoch 339/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0127 - acc: 0.9968 - val_loss: 0.0296 - val_acc: 0.9922\n",
      "Epoch 340/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.0227 - val_acc: 0.9916\n",
      "Epoch 341/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0094 - acc: 0.9962 - val_loss: 0.0335 - val_acc: 0.9888\n",
      "Epoch 342/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0083 - acc: 0.9971 - val_loss: 0.0249 - val_acc: 0.9922\n",
      "Epoch 343/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0256 - val_acc: 0.9927\n",
      "Epoch 344/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.0176 - val_acc: 0.9944\n",
      "Epoch 345/700\n",
      "7168/7168 [==============================] - 1s 87us/step - loss: 0.0098 - acc: 0.9961 - val_loss: 0.0239 - val_acc: 0.9894\n",
      "Epoch 346/700\n",
      "7168/7168 [==============================] - 1s 89us/step - loss: 0.0104 - acc: 0.9967 - val_loss: 0.0159 - val_acc: 0.9944\n",
      "Epoch 347/700\n",
      "7168/7168 [==============================] - 1s 89us/step - loss: 0.0109 - acc: 0.9971 - val_loss: 0.0167 - val_acc: 0.9944\n",
      "Epoch 348/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0280 - val_acc: 0.9911\n",
      "Epoch 349/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0274 - val_acc: 0.9900\n",
      "Epoch 350/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0336 - val_acc: 0.9911\n",
      "Epoch 351/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0319 - val_acc: 0.9888\n",
      "Epoch 352/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0304 - val_acc: 0.9905\n",
      "Epoch 353/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0105 - acc: 0.9964 - val_loss: 0.0311 - val_acc: 0.9877\n",
      "Epoch 354/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0431 - val_acc: 0.9855\n",
      "Epoch 355/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0373 - val_acc: 0.9883\n",
      "Epoch 356/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0110 - acc: 0.9967 - val_loss: 0.0353 - val_acc: 0.9883\n",
      "Epoch 357/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0301 - val_acc: 0.9922\n",
      "Epoch 358/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0112 - acc: 0.9957 - val_loss: 0.0407 - val_acc: 0.9872\n",
      "Epoch 359/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0067 - acc: 0.9973 - val_loss: 0.0274 - val_acc: 0.9888\n",
      "Epoch 360/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0279 - val_acc: 0.9922\n",
      "Epoch 361/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0129 - acc: 0.9953 - val_loss: 0.0307 - val_acc: 0.9900\n",
      "Epoch 362/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0268 - val_acc: 0.9905\n",
      "Epoch 363/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0116 - acc: 0.9958 - val_loss: 0.0367 - val_acc: 0.9900\n",
      "Epoch 364/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0088 - acc: 0.9975 - val_loss: 0.0169 - val_acc: 0.9944\n",
      "Epoch 365/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0115 - acc: 0.9972 - val_loss: 0.0222 - val_acc: 0.9939\n",
      "Epoch 366/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0149 - val_acc: 0.9955\n",
      "Epoch 367/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0112 - acc: 0.9961 - val_loss: 0.0254 - val_acc: 0.9905\n",
      "Epoch 368/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 369/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0113 - acc: 0.9960 - val_loss: 0.0299 - val_acc: 0.9905\n",
      "Epoch 370/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0338 - val_acc: 0.9877\n",
      "Epoch 371/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0115 - acc: 0.9955 - val_loss: 0.0328 - val_acc: 0.9900\n",
      "Epoch 372/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0372 - val_acc: 0.9877\n",
      "Epoch 373/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0229 - val_acc: 0.9922\n",
      "Epoch 374/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0267 - val_acc: 0.9916\n",
      "Epoch 375/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0207 - val_acc: 0.9933\n",
      "Epoch 376/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0097 - acc: 0.9967 - val_loss: 0.0347 - val_acc: 0.9883\n",
      "Epoch 377/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.0250 - val_acc: 0.9916\n",
      "Epoch 378/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0093 - acc: 0.9971 - val_loss: 0.0346 - val_acc: 0.9883\n",
      "Epoch 379/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0104 - acc: 0.9971 - val_loss: 0.0402 - val_acc: 0.9905\n",
      "Epoch 380/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0065 - acc: 0.9973 - val_loss: 0.0427 - val_acc: 0.9877\n",
      "Epoch 381/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0324 - val_acc: 0.9905\n",
      "Epoch 382/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0083 - acc: 0.9971 - val_loss: 0.0393 - val_acc: 0.9877\n",
      "Epoch 383/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0422 - val_acc: 0.9872\n",
      "Epoch 384/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0100 - acc: 0.9968 - val_loss: 0.0390 - val_acc: 0.9883\n",
      "Epoch 385/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0436 - val_acc: 0.9872\n",
      "Epoch 386/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0094 - acc: 0.9964 - val_loss: 0.0307 - val_acc: 0.9916\n",
      "Epoch 387/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0075 - acc: 0.9972 - val_loss: 0.0368 - val_acc: 0.9888\n",
      "Epoch 388/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.0206 - val_acc: 0.9922\n",
      "Epoch 389/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0162 - val_acc: 0.9933\n",
      "Epoch 390/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0221 - val_acc: 0.9900\n",
      "Epoch 391/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.0314 - val_acc: 0.9877\n",
      "Epoch 392/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0098 - acc: 0.9960 - val_loss: 0.0232 - val_acc: 0.9888\n",
      "Epoch 393/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0067 - acc: 0.9983 - val_loss: 0.0241 - val_acc: 0.9900\n",
      "Epoch 394/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0084 - acc: 0.9967 - val_loss: 0.0331 - val_acc: 0.9855\n",
      "Epoch 395/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0119 - acc: 0.9964 - val_loss: 0.0316 - val_acc: 0.9905\n",
      "Epoch 396/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0217 - val_acc: 0.9944\n",
      "Epoch 397/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0431 - val_acc: 0.9883\n",
      "Epoch 398/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0414 - val_acc: 0.9877\n",
      "Epoch 399/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0100 - acc: 0.9964 - val_loss: 0.0249 - val_acc: 0.9888\n",
      "Epoch 400/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0082 - acc: 0.9972 - val_loss: 0.0374 - val_acc: 0.9866\n",
      "Epoch 401/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0124 - acc: 0.9953 - val_loss: 0.0338 - val_acc: 0.9900\n",
      "Epoch 402/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0275 - val_acc: 0.9916\n",
      "Epoch 403/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0353 - val_acc: 0.9855\n",
      "Epoch 404/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.0309 - val_acc: 0.9877\n",
      "Epoch 405/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0250 - val_acc: 0.9927\n",
      "Epoch 406/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0232 - val_acc: 0.9939\n",
      "Epoch 407/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0260 - val_acc: 0.9905\n",
      "Epoch 408/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0249 - val_acc: 0.9905\n",
      "Epoch 409/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0101 - acc: 0.9957 - val_loss: 0.0470 - val_acc: 0.9872\n",
      "Epoch 410/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0114 - acc: 0.9955 - val_loss: 0.0220 - val_acc: 0.9916\n",
      "Epoch 411/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0219 - val_acc: 0.9922\n",
      "Epoch 412/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0100 - acc: 0.9960 - val_loss: 0.0299 - val_acc: 0.9905\n",
      "Epoch 413/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0088 - acc: 0.9967 - val_loss: 0.0296 - val_acc: 0.9883\n",
      "Epoch 414/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0269 - val_acc: 0.9939\n",
      "Epoch 415/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0107 - acc: 0.9961 - val_loss: 0.0309 - val_acc: 0.9888\n",
      "Epoch 416/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0256 - val_acc: 0.9944\n",
      "Epoch 417/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0313 - val_acc: 0.9911\n",
      "Epoch 418/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0306 - val_acc: 0.9888\n",
      "Epoch 419/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0076 - acc: 0.9979 - val_loss: 0.0171 - val_acc: 0.9939\n",
      "Epoch 420/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0236 - val_acc: 0.9944\n",
      "Epoch 421/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0290 - val_acc: 0.9916\n",
      "Epoch 422/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 423/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 424/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0078 - acc: 0.9969 - val_loss: 0.0326 - val_acc: 0.9900\n",
      "Epoch 425/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0368 - val_acc: 0.9888\n",
      "Epoch 426/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0470 - val_acc: 0.9849\n",
      "Epoch 427/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0065 - acc: 0.9973 - val_loss: 0.0522 - val_acc: 0.9833\n",
      "Epoch 428/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0320 - val_acc: 0.9905\n",
      "Epoch 429/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0318 - val_acc: 0.9900\n",
      "Epoch 430/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0257 - val_acc: 0.9939\n",
      "Epoch 431/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0297 - val_acc: 0.9911\n",
      "Epoch 432/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0280 - val_acc: 0.9911\n",
      "Epoch 433/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0232 - val_acc: 0.9955\n",
      "Epoch 434/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0182 - val_acc: 0.9955\n",
      "Epoch 435/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0158 - val_acc: 0.9967\n",
      "Epoch 436/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.0198 - val_acc: 0.9950\n",
      "Epoch 437/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0096 - acc: 0.9972 - val_loss: 0.0247 - val_acc: 0.9916\n",
      "Epoch 438/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0294 - val_acc: 0.9911\n",
      "Epoch 439/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0071 - acc: 0.9973 - val_loss: 0.0341 - val_acc: 0.9877\n",
      "Epoch 440/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0344 - val_acc: 0.9900\n",
      "Epoch 441/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0167 - val_acc: 0.9950\n",
      "Epoch 442/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0103 - acc: 0.9957 - val_loss: 0.0277 - val_acc: 0.9927\n",
      "Epoch 443/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0348 - val_acc: 0.9866\n",
      "Epoch 444/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 445/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0083 - acc: 0.9969 - val_loss: 0.0237 - val_acc: 0.9922\n",
      "Epoch 446/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0067 - acc: 0.9975 - val_loss: 0.0276 - val_acc: 0.9900\n",
      "Epoch 447/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0104 - acc: 0.9961 - val_loss: 0.0296 - val_acc: 0.9911\n",
      "Epoch 448/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0120 - acc: 0.9964 - val_loss: 0.0376 - val_acc: 0.9883\n",
      "Epoch 449/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0302 - val_acc: 0.9916\n",
      "Epoch 450/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0075 - acc: 0.9979 - val_loss: 0.0319 - val_acc: 0.9900\n",
      "Epoch 451/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0291 - val_acc: 0.9916\n",
      "Epoch 452/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.0289 - val_acc: 0.9911\n",
      "Epoch 453/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0104 - acc: 0.9957 - val_loss: 0.0289 - val_acc: 0.9911\n",
      "Epoch 454/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0209 - val_acc: 0.9944\n",
      "Epoch 455/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0277 - val_acc: 0.9927\n",
      "Epoch 456/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0053 - acc: 0.9975 - val_loss: 0.0269 - val_acc: 0.9922\n",
      "Epoch 457/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0055 - acc: 0.9976 - val_loss: 0.0207 - val_acc: 0.9927\n",
      "Epoch 458/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0320 - val_acc: 0.9894\n",
      "Epoch 459/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0276 - val_acc: 0.9905\n",
      "Epoch 460/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0071 - acc: 0.9973 - val_loss: 0.0201 - val_acc: 0.9955\n",
      "Epoch 461/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0073 - acc: 0.9971 - val_loss: 0.0307 - val_acc: 0.9888\n",
      "Epoch 462/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.0312 - val_acc: 0.9916\n",
      "Epoch 463/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0071 - acc: 0.9971 - val_loss: 0.0266 - val_acc: 0.9911\n",
      "Epoch 464/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0197 - val_acc: 0.9950\n",
      "Epoch 465/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0087 - acc: 0.9971 - val_loss: 0.0184 - val_acc: 0.9927\n",
      "Epoch 466/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0230 - val_acc: 0.9944\n",
      "Epoch 467/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0380 - val_acc: 0.9888\n",
      "Epoch 468/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0335 - val_acc: 0.9911\n",
      "Epoch 469/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0491 - val_acc: 0.9888\n",
      "Epoch 470/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0293 - val_acc: 0.9916\n",
      "Epoch 471/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0194 - val_acc: 0.9927\n",
      "Epoch 472/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0131 - acc: 0.9962 - val_loss: 0.0215 - val_acc: 0.9955\n",
      "Epoch 473/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0070 - acc: 0.9971 - val_loss: 0.0181 - val_acc: 0.9944\n",
      "Epoch 474/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0269 - val_acc: 0.9922\n",
      "Epoch 475/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0212 - val_acc: 0.9933\n",
      "Epoch 476/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0053 - acc: 0.9979 - val_loss: 0.0290 - val_acc: 0.9911\n",
      "Epoch 477/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0274 - val_acc: 0.9911\n",
      "Epoch 478/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0092 - acc: 0.9978 - val_loss: 0.0242 - val_acc: 0.9939\n",
      "Epoch 479/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0158 - val_acc: 0.9967\n",
      "Epoch 480/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0219 - val_acc: 0.9916\n",
      "Epoch 481/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0116 - acc: 0.9964 - val_loss: 0.0256 - val_acc: 0.9927\n",
      "Epoch 482/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0228 - val_acc: 0.9939\n",
      "Epoch 483/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0164 - val_acc: 0.9939\n",
      "Epoch 484/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0119 - val_acc: 0.9955\n",
      "Epoch 485/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0105 - acc: 0.9964 - val_loss: 0.0204 - val_acc: 0.9933\n",
      "Epoch 486/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0107 - acc: 0.9971 - val_loss: 0.0218 - val_acc: 0.9927\n",
      "Epoch 487/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0266 - val_acc: 0.9911\n",
      "Epoch 488/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0241 - val_acc: 0.9911\n",
      "Epoch 489/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0303 - val_acc: 0.9877\n",
      "Epoch 490/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0097 - acc: 0.9972 - val_loss: 0.0251 - val_acc: 0.9927\n",
      "Epoch 491/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0114 - acc: 0.9961 - val_loss: 0.0280 - val_acc: 0.9905\n",
      "Epoch 492/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0109 - acc: 0.9958 - val_loss: 0.0367 - val_acc: 0.9883\n",
      "Epoch 493/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0117 - acc: 0.9958 - val_loss: 0.0364 - val_acc: 0.9883\n",
      "Epoch 494/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0331 - val_acc: 0.9894\n",
      "Epoch 495/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0330 - val_acc: 0.9905\n",
      "Epoch 496/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0063 - acc: 0.9973 - val_loss: 0.0221 - val_acc: 0.9944\n",
      "Epoch 497/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0076 - acc: 0.9971 - val_loss: 0.0270 - val_acc: 0.9927\n",
      "Epoch 498/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0204 - val_acc: 0.9933\n",
      "Epoch 499/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0390 - val_acc: 0.9888\n",
      "Epoch 500/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0274 - val_acc: 0.9922\n",
      "Epoch 501/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0376 - val_acc: 0.9872\n",
      "Epoch 502/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0307 - val_acc: 0.9922\n",
      "Epoch 503/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0063 - acc: 0.9975 - val_loss: 0.0241 - val_acc: 0.9922\n",
      "Epoch 504/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0159 - val_acc: 0.9950\n",
      "Epoch 505/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0210 - val_acc: 0.9950\n",
      "Epoch 506/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0269 - val_acc: 0.9927\n",
      "Epoch 507/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0247 - val_acc: 0.9933\n",
      "Epoch 508/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0073 - acc: 0.9983 - val_loss: 0.0265 - val_acc: 0.9905\n",
      "Epoch 509/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0066 - acc: 0.9972 - val_loss: 0.0248 - val_acc: 0.9927\n",
      "Epoch 510/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0317 - val_acc: 0.9888\n",
      "Epoch 511/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.0319 - val_acc: 0.9922\n",
      "Epoch 512/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0083 - acc: 0.9979 - val_loss: 0.0308 - val_acc: 0.9888\n",
      "Epoch 513/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0256 - val_acc: 0.9894\n",
      "Epoch 514/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0233 - val_acc: 0.9927\n",
      "Epoch 515/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 516/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0372 - val_acc: 0.9877\n",
      "Epoch 517/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0058 - acc: 0.9972 - val_loss: 0.0368 - val_acc: 0.9877\n",
      "Epoch 518/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.0228 - val_acc: 0.9916\n",
      "Epoch 519/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0191 - val_acc: 0.9927\n",
      "Epoch 520/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0294 - val_acc: 0.9927\n",
      "Epoch 521/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0302 - val_acc: 0.9894\n",
      "Epoch 522/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0319 - val_acc: 0.9905\n",
      "Epoch 523/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0066 - acc: 0.9973 - val_loss: 0.0297 - val_acc: 0.9927\n",
      "Epoch 524/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0255 - val_acc: 0.9916\n",
      "Epoch 525/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0071 - acc: 0.9973 - val_loss: 0.0272 - val_acc: 0.9927\n",
      "Epoch 526/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0068 - acc: 0.9973 - val_loss: 0.0244 - val_acc: 0.9933\n",
      "Epoch 527/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0184 - val_acc: 0.9939\n",
      "Epoch 528/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0167 - val_acc: 0.9950\n",
      "Epoch 529/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0240 - val_acc: 0.9933\n",
      "Epoch 530/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0076 - acc: 0.9971 - val_loss: 0.0190 - val_acc: 0.9922\n",
      "Epoch 531/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0196 - val_acc: 0.9916\n",
      "Epoch 532/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0254 - val_acc: 0.9911\n",
      "Epoch 533/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0205 - val_acc: 0.9922\n",
      "Epoch 534/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0733 - val_acc: 0.9777\n",
      "Epoch 535/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0171 - val_acc: 0.9939\n",
      "Epoch 536/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0221 - val_acc: 0.9916\n",
      "Epoch 537/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0208 - val_acc: 0.9922\n",
      "Epoch 538/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0236 - val_acc: 0.9927\n",
      "Epoch 539/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.0223 - val_acc: 0.9911\n",
      "Epoch 540/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0256 - val_acc: 0.9933\n",
      "Epoch 541/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0253 - val_acc: 0.9922\n",
      "Epoch 542/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0204 - val_acc: 0.9927\n",
      "Epoch 543/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0203 - val_acc: 0.9911\n",
      "Epoch 544/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0231 - val_acc: 0.9933\n",
      "Epoch 545/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.0273 - val_acc: 0.9922\n",
      "Epoch 546/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0300 - val_acc: 0.9916\n",
      "Epoch 547/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0306 - val_acc: 0.9922\n",
      "Epoch 548/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0060 - acc: 0.9976 - val_loss: 0.0274 - val_acc: 0.9905\n",
      "Epoch 549/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0224 - val_acc: 0.9927\n",
      "Epoch 550/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0087 - acc: 0.9968 - val_loss: 0.0182 - val_acc: 0.9944\n",
      "Epoch 551/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0152 - val_acc: 0.9933\n",
      "Epoch 552/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0074 - acc: 0.9972 - val_loss: 0.0293 - val_acc: 0.9900\n",
      "Epoch 553/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0063 - acc: 0.9976 - val_loss: 0.0215 - val_acc: 0.9927\n",
      "Epoch 554/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0170 - val_acc: 0.9939\n",
      "Epoch 555/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0074 - acc: 0.9972 - val_loss: 0.0260 - val_acc: 0.9900\n",
      "Epoch 556/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0202 - val_acc: 0.9916\n",
      "Epoch 557/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0232 - val_acc: 0.9939\n",
      "Epoch 558/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0180 - val_acc: 0.9955\n",
      "Epoch 559/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0160 - val_acc: 0.9950\n",
      "Epoch 560/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0094 - acc: 0.9972 - val_loss: 0.0217 - val_acc: 0.9933\n",
      "Epoch 561/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0208 - val_acc: 0.9944\n",
      "Epoch 562/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0185 - val_acc: 0.9939\n",
      "Epoch 563/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0034 - acc: 0.9985 - val_loss: 0.0191 - val_acc: 0.9939\n",
      "Epoch 564/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0214 - val_acc: 0.9944\n",
      "Epoch 565/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0127 - val_acc: 0.9944\n",
      "Epoch 566/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0194 - val_acc: 0.9922\n",
      "Epoch 567/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0063 - acc: 0.9976 - val_loss: 0.0198 - val_acc: 0.9939\n",
      "Epoch 568/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0295 - val_acc: 0.9900\n",
      "Epoch 569/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0275 - val_acc: 0.9916\n",
      "Epoch 570/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0318 - val_acc: 0.9905\n",
      "Epoch 571/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0139 - val_acc: 0.9950\n",
      "Epoch 572/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.0236 - val_acc: 0.9927\n",
      "Epoch 573/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.0242 - val_acc: 0.9916\n",
      "Epoch 574/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.0175 - val_acc: 0.9933\n",
      "Epoch 575/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9980 - val_loss: 0.0302 - val_acc: 0.9905\n",
      "Epoch 576/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0210 - val_acc: 0.9944\n",
      "Epoch 577/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0177 - val_acc: 0.9944\n",
      "Epoch 578/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0316 - val_acc: 0.9911\n",
      "Epoch 579/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0335 - val_acc: 0.9911\n",
      "Epoch 580/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0238 - val_acc: 0.9900\n",
      "Epoch 581/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0069 - acc: 0.9968 - val_loss: 0.0257 - val_acc: 0.9911\n",
      "Epoch 582/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0207 - val_acc: 0.9927\n",
      "Epoch 583/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0111 - acc: 0.9958 - val_loss: 0.0354 - val_acc: 0.9883\n",
      "Epoch 584/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0243 - val_acc: 0.9922\n",
      "Epoch 585/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0067 - acc: 0.9973 - val_loss: 0.0262 - val_acc: 0.9888\n",
      "Epoch 586/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0166 - val_acc: 0.9950\n",
      "Epoch 587/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0060 - acc: 0.9978 - val_loss: 0.0221 - val_acc: 0.9927\n",
      "Epoch 588/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0066 - acc: 0.9975 - val_loss: 0.0196 - val_acc: 0.9944\n",
      "Epoch 589/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0212 - val_acc: 0.9939\n",
      "Epoch 590/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0216 - val_acc: 0.9922\n",
      "Epoch 591/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0085 - acc: 0.9965 - val_loss: 0.0291 - val_acc: 0.9933\n",
      "Epoch 592/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.0240 - val_acc: 0.9933\n",
      "Epoch 593/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0251 - val_acc: 0.9933\n",
      "Epoch 594/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0213 - val_acc: 0.9944\n",
      "Epoch 595/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0272 - val_acc: 0.9911\n",
      "Epoch 596/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.0176 - val_acc: 0.9955\n",
      "Epoch 597/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0262 - val_acc: 0.9922\n",
      "Epoch 598/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.0334 - val_acc: 0.9911\n",
      "Epoch 599/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0270 - val_acc: 0.9905\n",
      "Epoch 600/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0358 - val_acc: 0.9894\n",
      "Epoch 601/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0155 - val_acc: 0.9927\n",
      "Epoch 602/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0204 - val_acc: 0.9922\n",
      "Epoch 603/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0129 - val_acc: 0.9950\n",
      "Epoch 604/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0194 - val_acc: 0.9922\n",
      "Epoch 605/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0089 - acc: 0.9979 - val_loss: 0.0197 - val_acc: 0.9916\n",
      "Epoch 606/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0160 - val_acc: 0.9944\n",
      "Epoch 607/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0203 - val_acc: 0.9927\n",
      "Epoch 608/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0355 - val_acc: 0.9888\n",
      "Epoch 609/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0252 - val_acc: 0.9927\n",
      "Epoch 610/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0062 - acc: 0.9975 - val_loss: 0.0223 - val_acc: 0.9922\n",
      "Epoch 611/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0085 - acc: 0.9967 - val_loss: 0.0157 - val_acc: 0.9944\n",
      "Epoch 612/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0101 - val_acc: 0.9967\n",
      "Epoch 613/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0085 - acc: 0.9973 - val_loss: 0.0161 - val_acc: 0.9927\n",
      "Epoch 614/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.0190 - val_acc: 0.9927\n",
      "Epoch 615/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0086 - acc: 0.9969 - val_loss: 0.0228 - val_acc: 0.9933\n",
      "Epoch 616/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0091 - acc: 0.9979 - val_loss: 0.0186 - val_acc: 0.9939\n",
      "Epoch 617/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0212 - val_acc: 0.9933\n",
      "Epoch 618/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0205 - val_acc: 0.9944\n",
      "Epoch 619/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0282 - val_acc: 0.9927\n",
      "Epoch 620/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0199 - val_acc: 0.9950\n",
      "Epoch 621/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0052 - acc: 0.9978 - val_loss: 0.0160 - val_acc: 0.9961\n",
      "Epoch 622/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0249 - val_acc: 0.9933\n",
      "Epoch 623/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0184 - val_acc: 0.9961\n",
      "Epoch 624/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0242 - val_acc: 0.9905\n",
      "Epoch 625/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0245 - val_acc: 0.9933\n",
      "Epoch 626/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0243 - val_acc: 0.9922\n",
      "Epoch 627/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0208 - val_acc: 0.9933\n",
      "Epoch 628/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 0.0162 - val_acc: 0.9944\n",
      "Epoch 629/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0229 - val_acc: 0.9911\n",
      "Epoch 630/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0167 - val_acc: 0.9922\n",
      "Epoch 631/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0060 - acc: 0.9975 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 632/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0257 - val_acc: 0.9916\n",
      "Epoch 633/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0232 - val_acc: 0.9922\n",
      "Epoch 634/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0230 - val_acc: 0.9911\n",
      "Epoch 635/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 636/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0234 - val_acc: 0.9911\n",
      "Epoch 637/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0275 - val_acc: 0.9916\n",
      "Epoch 638/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0107 - acc: 0.9967 - val_loss: 0.0170 - val_acc: 0.9939\n",
      "Epoch 639/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0223 - val_acc: 0.9916\n",
      "Epoch 640/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0065 - acc: 0.9975 - val_loss: 0.0297 - val_acc: 0.9905\n",
      "Epoch 641/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0320 - val_acc: 0.9916\n",
      "Epoch 642/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0073 - acc: 0.9972 - val_loss: 0.0278 - val_acc: 0.9916\n",
      "Epoch 643/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0171 - val_acc: 0.9944\n",
      "Epoch 644/700\n",
      "7168/7168 [==============================] - 1s 80us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0186 - val_acc: 0.9927\n",
      "Epoch 645/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0237 - val_acc: 0.9905\n",
      "Epoch 646/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0372 - val_acc: 0.9844\n",
      "Epoch 647/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0337 - val_acc: 0.9916\n",
      "Epoch 648/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0329 - val_acc: 0.9916\n",
      "Epoch 649/700\n",
      "7168/7168 [==============================] - 1s 85us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0350 - val_acc: 0.9900\n",
      "Epoch 650/700\n",
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0058 - acc: 0.9975 - val_loss: 0.0248 - val_acc: 0.9916\n",
      "Epoch 651/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/7168 [==============================] - 1s 88us/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0228 - val_acc: 0.9916\n",
      "Epoch 652/700\n",
      "7168/7168 [==============================] - 1s 91us/step - loss: 0.0050 - acc: 0.9976 - val_loss: 0.0218 - val_acc: 0.9944\n",
      "Epoch 653/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9973 - val_loss: 0.0194 - val_acc: 0.9933\n",
      "Epoch 654/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0249 - val_acc: 0.9933\n",
      "Epoch 655/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0204 - val_acc: 0.9933\n",
      "Epoch 656/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0227 - val_acc: 0.9939\n",
      "Epoch 657/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0168 - val_acc: 0.9955\n",
      "Epoch 658/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0137 - val_acc: 0.9961\n",
      "Epoch 659/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0292 - val_acc: 0.9933\n",
      "Epoch 660/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0164 - val_acc: 0.9961\n",
      "Epoch 661/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0164 - val_acc: 0.9939\n",
      "Epoch 662/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0068 - acc: 0.9973 - val_loss: 0.0192 - val_acc: 0.9939\n",
      "Epoch 663/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0058 - acc: 0.9975 - val_loss: 0.0208 - val_acc: 0.9944\n",
      "Epoch 664/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0222 - val_acc: 0.9933\n",
      "Epoch 665/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0264 - val_acc: 0.9905\n",
      "Epoch 666/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0283 - val_acc: 0.9916\n",
      "Epoch 667/700\n",
      "7168/7168 [==============================] - 1s 86us/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0191 - val_acc: 0.9944\n",
      "Epoch 668/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0206 - val_acc: 0.9950\n",
      "Epoch 669/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0209 - val_acc: 0.9922\n",
      "Epoch 670/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0090 - acc: 0.9980 - val_loss: 0.0321 - val_acc: 0.9916\n",
      "Epoch 671/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0059 - acc: 0.9972 - val_loss: 0.0433 - val_acc: 0.9888\n",
      "Epoch 672/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.0319 - val_acc: 0.9916\n",
      "Epoch 673/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0444 - val_acc: 0.9888\n",
      "Epoch 674/700\n",
      "7168/7168 [==============================] - 1s 84us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0437 - val_acc: 0.9911\n",
      "Epoch 675/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0426 - val_acc: 0.9883\n",
      "Epoch 676/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0214 - val_acc: 0.9916\n",
      "Epoch 677/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 678/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0066 - acc: 0.9975 - val_loss: 0.0263 - val_acc: 0.9950\n",
      "Epoch 679/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0092 - acc: 0.9979 - val_loss: 0.0254 - val_acc: 0.9939\n",
      "Epoch 680/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0333 - val_acc: 0.9927\n",
      "Epoch 681/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0286 - val_acc: 0.9911\n",
      "Epoch 682/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0095 - acc: 0.9972 - val_loss: 0.0267 - val_acc: 0.9950\n",
      "Epoch 683/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0269 - val_acc: 0.9894\n",
      "Epoch 684/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0113 - acc: 0.9971 - val_loss: 0.0311 - val_acc: 0.9905\n",
      "Epoch 685/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0265 - val_acc: 0.9883\n",
      "Epoch 686/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0311 - val_acc: 0.9888\n",
      "Epoch 687/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0205 - val_acc: 0.9927\n",
      "Epoch 688/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0249 - val_acc: 0.9916\n",
      "Epoch 689/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0223 - val_acc: 0.9916\n",
      "Epoch 690/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 691/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0386 - val_acc: 0.9888\n",
      "Epoch 692/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0274 - val_acc: 0.9905\n",
      "Epoch 693/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0058 - acc: 0.9978 - val_loss: 0.0381 - val_acc: 0.9905\n",
      "Epoch 694/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0237 - val_acc: 0.9927\n",
      "Epoch 695/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0230 - val_acc: 0.9922\n",
      "Epoch 696/700\n",
      "7168/7168 [==============================] - 1s 81us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0225 - val_acc: 0.9927\n",
      "Epoch 697/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0280 - val_acc: 0.9905\n",
      "Epoch 698/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0060 - acc: 0.9973 - val_loss: 0.0159 - val_acc: 0.9944\n",
      "Epoch 699/700\n",
      "7168/7168 [==============================] - 1s 83us/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0200 - val_acc: 0.9939\n",
      "Epoch 700/700\n",
      "7168/7168 [==============================] - 1s 82us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0224 - val_acc: 0.9922\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=700\n",
    "\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=0, mode='auto')\n",
    "\n",
    "#训练\n",
    "history = model.fit(X_train, encoded_y_train, batch_size=batch_size,shuffle=True, epochs=epochs, validation_split=0.2)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# 可视化valication trend\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 可视化loss trend\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:0.9939759036144579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = np.rint(model.predict(X_test))\n",
    "print(f'Accuracy Score:{accuracy_score(encoded_y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[249   0   0   2]\n",
      " [  0 265   0   0]\n",
      " [  0   0 237   0]\n",
      " [  3   1   0 239]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_unencoded = np.argmax(y_pred, axis=1)\n",
    "y_test_unencoded = np.argmax(encoded_y_test, axis=1)\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test_unencoded, y_pred_unencoded)}')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
